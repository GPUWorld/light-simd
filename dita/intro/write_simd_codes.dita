<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="write_simd_codes">
  <title>Write SIMD Codes</title>
  <body>
    <p>Whereas modern CPUs provide SIMD instruction set architecture, your program, if written in
      traditional way, would not <i>magically</i> use SIMD. To leverage the efficiency of SIMD
      instructions, you have to explicitly write SIMD codes. If you are using C/C++, there are three
      ways to do this:</p>
    <section>
      <title>Inline assembly</title>
      <p>One can take advantage of the inline assembly syntax in C/C++ to directly invoke SIMD
        instructions. Here is a good <xref
          href="http://www.3dbuzz.com/vbforum/showthread.php?104753-HowTo-Inline-Assembly-amp-SSE-Vector-normalization-done-fast!"
          format="php" scope="external">tutorial</xref> on how to write SSE codes via inline
        assembly. Generally, this requires very low-level stuff such as hand-coded register
        allocation and stack management, and it is nontrivial to write portable ASM codes.</p>
    </section>
    <section>
      <title>Intrinsics</title>
      <p>An <b>intrinsic</b> is a built-in function that the compiler would directly maps to one or
        more assembly instructions. Intel specifies a large collection of intrinsics for SSE and
        AVX, which many modern compilers provide (nearly) complete support. As an example to
        demonstrate the use of SSE intrinsics, the following is a simple function that calculates 
        <codeph>y += a * x</codeph> on arrays with n entries (here, we simply assume n is a multiple of 4 for
        simplicity).</p>
      <codeblock><keyword>void</keyword> <apiname>vadd_prod_f32</apiname>(<keyword>unsigned</keyword> n, <keyword>float</keyword> a, <keyword>const float</keyword> *x, <keyword>float</keyword> *y)
{
    <keyword>__m128</keyword> pa = <apiname>_mm_set1_ps</apiname>(a);

    <keyword>unsigned</keyword> m = n / 4;
    <keyword>for</keyword> (<keyword>unsigned</keyword> i = 0; i &lt; m; ++i, x += 4, y += 4)
    {
        <keyword>__m128</keyword> px = <apiname>_mm_loadu_ps</apiname>(x);
        <keyword>__m128</keyword> py = <apiname>_mm_loadu_ps</apiname>(y);
    
        py = <apiname>_mm_add_ps</apiname>(py, <apiname>_mm_mul_ps</apiname>(pa, px));
        <apiname>_mm_storeu_ps</apiname>(y, py);
    } 
}</codeblock>
      <p>A modified function can be used for double-precision numbers:</p>
      <codeblock><keyword>void</keyword> <apiname>vadd_prod_f64</apiname>(<keyword>unsigned</keyword> n, <keyword>double</keyword> a, <keyword>const double</keyword> *x, <keyword>double</keyword> *y)
{
    <keyword>__m128d</keyword> pa = <apiname>_mm_set1_pd</apiname>(a);

    <keyword>unsigned</keyword> m = n / 2;
    <keyword>for</keyword> (<keyword>unsigned</keyword> i = 0; i &lt; m; ++i, x += 2, y += 2)
    {
        <keyword>__m128d</keyword> px = <apiname>_mm_loadu_pd</apiname>(x);
        <keyword>__m128d</keyword> py = <apiname>_mm_loadu_pd</apiname>(y);
    
        py = <apiname>_mm_add_pd</apiname>(py, <apiname>_mm_mul_pd</apiname>(pa, px));
        <apiname>_mm_storeu_pd</apiname>(y, py);
    } 
}</codeblock>
      <p>Compared to inline assembly, the codes using intrinsics are easier to read and write.
        However, this way is not very satisfactory. It is limited in several aspects: </p>
      <ul>
        <li>The SSE data types and function names for floats and doubles are different. The same
          task need to be implemented for different data types respectively.</li>
        <li>The intrinsic function names such as <codeph><apiname>_mm_loadu_ps</apiname></codeph>
          and <codeph><apiname>_mm_set1_pd</apiname></codeph> are not very intuitive.</li>
        <li>The functions above are <i>not portable</i> to other architectures, such as AVX.
          Basically, one needs to re-implement the functions under different architectures.</li>
      </ul>
    </section>
    <section>
      <title>High-level generic library</title>
      <p>Through C++ template specialization mechanism, <xref
          href="main_features.dita#main_features">Light-SIMD</xref> provides generic interfaces for
        SIMD programming, where the architecture-dependent details are encapsulated. With
        Light-SIMD, users can write portable SIMD codes much more easily and cleanly. Below shows
        how a generic function to calculate <codeph>y += a * x</codeph> can be implemented using
        Light-SIMD.</p>
      <codeblock><keyword>using namespace</keyword> lsimd;

<keyword>template</keyword>&lt;<keyword>typename</keyword> T>
<keyword>void</keyword> <apiname>vadd_prod</apiname>(<keyword>unsigned</keyword> n, T a, <keyword>const</keyword> T *x, T *y)
{
    <keyword>const unsigned</keyword> wid = <apiname>simd</apiname>&lt;T>::pack_width;
    <keyword>const unsigned</keyword> m = n / wid;
   
    <apiname>simd_pack</apiname>&lt;T> pa(x, <apiname>unaligned_t</apiname>());
    <keyword>for</keyword> (<keyword>unsigned</keyword> i = 0; i &lt; m; ++i, x += wid, y += wid)
    {
        <apiname>simd_pack</apiname>&lt;T> px(x, <apiname>unaligned_t</apiname>());
        <apiname>simd_pack</apiname>&lt;T> py(y, <apiname>unaligned_t</apiname>());
       
        py += pa * px;
        py.<apiname>store</apiname>(y, <apiname>unaligned_t</apiname>());
    }
}</codeblock>
      <p>The code here is cleaner. More importantly, it is portable and can be used for different
        data types and on different instruction set architectures. For example,</p>
        <codeblock><keyword>float</keyword> af, *xf, *yf; 
<keyword>double</keyword> ad, *xd, *yd;

<i>// *** some code to initialize the variables</i>

<apiname>vadd_prod</apiname>(n, af, xf, yf);
<apiname>vadd_prod</apiname>(n, ad, xd, yd);</codeblock>
      <p>The library will resolve the proper instruction set architecture to use, depending on
        compilation settings. This frees the developer of the tedious job of writing low-level
        architecture-dependent codes. </p>
    </section>
  </body>
</topic>
